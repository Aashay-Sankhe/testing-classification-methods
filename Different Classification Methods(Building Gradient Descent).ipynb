{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f6964e",
   "metadata": {},
   "source": [
    "## Testing Various Methods of Binary Classification\n",
    "### We will be testing a gradient descent implementation from scratch, a standardized descent, and a neural network to compare metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "500b9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24752b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining functions for classification and regression tasks, the following methods are needed:\n",
    "\n",
    "# 1. A method to calculate the prediction of our model given certain instantiated params (weights and bias)\n",
    "# 2. A method to calculate our model's accuracy (calculate the loss of the current model using logistic loss)\n",
    "# 3. A method to calculate a single step of gradient descent in our model\n",
    "# 4. A method to iteratively perform gradient descent to minimize our loss function\n",
    "# 5. A method for calculating the accuracy of our model's prediction after training\n",
    "\n",
    "def predict(w, b, x):\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    z = sigmoid(np.dot(x,w)+b)\n",
    "    return z\n",
    "\n",
    "def loss(w, b, X, y, predict):\n",
    "    z = predict(w, b, X)\n",
    "    loss = -np.mean(y * np.log(z + 1e-10) + (1 - y) * np.log(1 - z + 1e-10))\n",
    "    return loss\n",
    "\n",
    "def gradient_step(w, b, X, y, predict):\n",
    "    m = len(X)\n",
    "    predictions = predict(w, b, X)\n",
    "    err = predictions - y\n",
    "    d_dw = np.dot(X.T, err) / m\n",
    "    d_db = np.mean(err)\n",
    "    return d_dw, d_db\n",
    "\n",
    "def iterations(w, b, X, y, predict, gradient_step, iters, alpha, loss):\n",
    "    for i in range(iters):\n",
    "        dw, db = gradient_step(w, b, X, y, predict)\n",
    "        w -= (alpha * dw)\n",
    "        b -= (alpha * db)\n",
    "        if i % 5000 == 0:print(\"Current loss: {loss}\".format(loss=loss(w, b, X, y, predict)))\n",
    "    return w, b\n",
    "def check(X, predict, y, w, b):\n",
    "    num_incorrect = 0\n",
    "    for i in range(len(X)):\n",
    "        z = predict(w, b, X[i])\n",
    "        if z >=0.5: z = 1\n",
    "        else: z = 0\n",
    "        if z != y[i]: \n",
    "            num_incorrect +=1\n",
    "    acc = ((len(x_test) - num_incorrect) / len(x_test)) * 100\n",
    "    return num_incorrect, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca96917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample data for testing\n",
    "#The data is accumulated and cleaned, removing null values, irrelevant values, and features that are in direct correlation with each other.\n",
    "\n",
    "data1, data2 = pd.read_csv(\"Data1.csv\"), pd.read_csv(\"Data2.csv\")\n",
    "all_data = data1.merge(data2, how='inner',on='ID')\n",
    "all_data = all_data.drop(columns='ID')\n",
    "all_data = all_data.dropna()\n",
    "all_data = all_data.drop(columns='Age')\n",
    "all_data = all_data.drop(columns='ZipCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10a394e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the training and testing split using sklearn\n",
    "#We also have to convert the dataframes into numpy arrays\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = all_data['LoanOnCard']\n",
    "X = all_data.drop(columns='LoanOnCard')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1)\n",
    "x_train, x_test, y_train, y_test = x_train.to_numpy(), x_test.to_numpy(), y_train.to_numpy(), y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "424359c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss: 0.6484622116359611\n",
      "Current loss: 0.31496412759178166\n",
      "Current loss: 0.3050743990781503\n",
      "Current loss: 0.29771040946457733\n",
      "Current loss: 0.2910618163580063\n",
      "Current loss: 0.28488108925272426\n",
      "Current loss: 0.27908420869144046\n",
      "Current loss: 0.27362028041167097\n",
      "Current loss: 0.2684510179420149\n",
      "Current loss: 0.2635453671869523\n"
     ]
    }
   ],
   "source": [
    "#Initializing parameters and beginning gradient descent iterations:\n",
    "\n",
    "w_in, b_in = np.zeros_like(x_train[0]), 0\n",
    "w, b = iterations(w_in, b_in, x_train, y_train, predict, gradient_step, 50000, 0.001, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "476e3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the accuracy of the model on the testing set:\n",
    "init_incorrect, init_acc = check(x_test, predict, y_test, w, b)\n",
    "init_loss = loss(w, b, x_test, y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a82a820a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss: 0.7680205212435258\n",
      "Current loss: 0.3057746435306232\n",
      "Current loss: 0.295802577744056\n",
      "Current loss: 0.28989834587407415\n",
      "Current loss: 0.2858760754448538\n",
      "Current loss: 0.28295153233979836\n",
      "Current loss: 0.28072976391908283\n",
      "Current loss: 0.2789853548810546\n",
      "Current loss: 0.2775796295274175\n",
      "Current loss: 0.2764224695808323\n"
     ]
    }
   ],
   "source": [
    "x_train_standardized = np.zeros_like(x_train)\n",
    "for i in range(len(x_train)):\n",
    "    mean = np.mean(x_train[i])\n",
    "    std = np.std(x_train[i])\n",
    "    x_train_standardized[i] = (x_train[i] - mean)/std\n",
    "w_s, b_s =  iterations(w_in, b_in, x_train_standardized, y_train, predict, gradient_step, 50000, 0.001, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "74a914bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the accuracy of the standardized model on the testing set:\n",
    "s_incorrect, s_acc = check(x_test, predict, y_test, w_s, b_s)\n",
    "s_loss = loss(w_s, b_s, x_test, y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ca4f752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - Accuracy: 0.9022 - loss: 0.7836\n",
      "Epoch 2/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9090 - loss: 0.3404\n",
      "Epoch 3/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9020 - loss: 0.2986\n",
      "Epoch 4/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.8980 - loss: 0.2841\n",
      "Epoch 5/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9038 - loss: 0.2573\n",
      "Epoch 6/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9045 - loss: 0.2390\n",
      "Epoch 7/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9033 - loss: 0.2385\n",
      "Epoch 8/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9007 - loss: 0.2237\n",
      "Epoch 9/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9013 - loss: 0.2214\n",
      "Epoch 10/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9091 - loss: 0.2104\n",
      "Epoch 11/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9017 - loss: 0.2247\n",
      "Epoch 12/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9012 - loss: 0.2146\n",
      "Epoch 13/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9065 - loss: 0.2111\n",
      "Epoch 14/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9106 - loss: 0.1907\n",
      "Epoch 15/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9062 - loss: 0.1993\n",
      "Epoch 16/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9036 - loss: 0.2093\n",
      "Epoch 17/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9042 - loss: 0.2009\n",
      "Epoch 18/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9070 - loss: 0.2033\n",
      "Epoch 19/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9031 - loss: 0.2039\n",
      "Epoch 20/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9148 - loss: 0.1861\n",
      "Epoch 21/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9058 - loss: 0.1962\n",
      "Epoch 22/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9066 - loss: 0.1924\n",
      "Epoch 23/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9044 - loss: 0.1933\n",
      "Epoch 24/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9019 - loss: 0.2043\n",
      "Epoch 25/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9061 - loss: 0.1891\n",
      "Epoch 26/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9046 - loss: 0.1989\n",
      "Epoch 27/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9121 - loss: 0.1844\n",
      "Epoch 28/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9033 - loss: 0.2019\n",
      "Epoch 29/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9080 - loss: 0.1897\n",
      "Epoch 30/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9010 - loss: 0.2001\n",
      "Epoch 31/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9054 - loss: 0.1973\n",
      "Epoch 32/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9132 - loss: 0.1799\n",
      "Epoch 33/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9086 - loss: 0.1826\n",
      "Epoch 34/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9001 - loss: 0.1901\n",
      "Epoch 35/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9093 - loss: 0.1776\n",
      "Epoch 36/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9051 - loss: 0.1908\n",
      "Epoch 37/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9046 - loss: 0.1889\n",
      "Epoch 38/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9110 - loss: 0.1886\n",
      "Epoch 39/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.8926 - loss: 0.1996\n",
      "Epoch 40/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9024 - loss: 0.1793\n",
      "Epoch 41/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.8975 - loss: 0.2005\n",
      "Epoch 42/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9071 - loss: 0.1831\n",
      "Epoch 43/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9061 - loss: 0.1932\n",
      "Epoch 44/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9061 - loss: 0.1711\n",
      "Epoch 45/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9065 - loss: 0.1791\n",
      "Epoch 46/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9016 - loss: 0.1853\n",
      "Epoch 47/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9079 - loss: 0.1760\n",
      "Epoch 48/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9126 - loss: 0.1692\n",
      "Epoch 49/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9104 - loss: 0.1834\n",
      "Epoch 50/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9155 - loss: 0.1633\n",
      "Epoch 51/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9157 - loss: 0.1656\n",
      "Epoch 52/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.8954 - loss: 0.1919\n",
      "Epoch 53/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9064 - loss: 0.1897\n",
      "Epoch 54/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9186 - loss: 0.1694\n",
      "Epoch 55/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9160 - loss: 0.1670\n",
      "Epoch 56/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9045 - loss: 0.1813\n",
      "Epoch 57/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9070 - loss: 0.1740\n",
      "Epoch 58/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9091 - loss: 0.1749\n",
      "Epoch 59/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9082 - loss: 0.1764\n",
      "Epoch 60/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9158 - loss: 0.1713\n",
      "Epoch 61/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9179 - loss: 0.1567\n",
      "Epoch 62/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9045 - loss: 0.1742\n",
      "Epoch 63/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9055 - loss: 0.1833\n",
      "Epoch 64/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9085 - loss: 0.1718\n",
      "Epoch 65/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9210 - loss: 0.1608\n",
      "Epoch 66/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9174 - loss: 0.1691\n",
      "Epoch 67/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9110 - loss: 0.1673\n",
      "Epoch 68/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9110 - loss: 0.1757\n",
      "Epoch 69/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9211 - loss: 0.1499\n",
      "Epoch 70/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9133 - loss: 0.1780\n",
      "Epoch 71/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9243 - loss: 0.1559\n",
      "Epoch 72/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9326 - loss: 0.1414\n",
      "Epoch 73/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9344 - loss: 0.1408\n",
      "Epoch 74/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9328 - loss: 0.1430\n",
      "Epoch 75/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9420 - loss: 0.1298\n",
      "Epoch 76/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9369 - loss: 0.1415\n",
      "Epoch 77/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9487 - loss: 0.1193\n",
      "Epoch 78/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9396 - loss: 0.1373\n",
      "Epoch 79/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9369 - loss: 0.1424\n",
      "Epoch 80/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9398 - loss: 0.1377\n",
      "Epoch 81/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9449 - loss: 0.1316\n",
      "Epoch 82/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9388 - loss: 0.1394\n",
      "Epoch 83/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9503 - loss: 0.1148\n",
      "Epoch 84/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9515 - loss: 0.1116\n",
      "Epoch 85/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9448 - loss: 0.1236\n",
      "Epoch 86/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9352 - loss: 0.1471\n",
      "Epoch 87/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9457 - loss: 0.1186\n",
      "Epoch 88/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9472 - loss: 0.1153\n",
      "Epoch 89/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9413 - loss: 0.1243\n",
      "Epoch 90/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9464 - loss: 0.1281\n",
      "Epoch 91/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9503 - loss: 0.1151\n",
      "Epoch 92/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9508 - loss: 0.1162\n",
      "Epoch 93/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9473 - loss: 0.1177\n",
      "Epoch 94/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9459 - loss: 0.1320\n",
      "Epoch 95/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9474 - loss: 0.1211\n",
      "Epoch 96/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9497 - loss: 0.1089\n",
      "Epoch 97/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9483 - loss: 0.1131\n",
      "Epoch 98/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9424 - loss: 0.1222\n",
      "Epoch 99/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - Accuracy: 0.9501 - loss: 0.1299\n",
      "Epoch 100/100\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - Accuracy: 0.9396 - loss: 0.1261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2737f0b8fd0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Neural Network Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "nn_model = Sequential([\n",
    "    Dense(units=12, activation=tf.nn.relu),\n",
    "    Dense(units=12, activation=tf.nn.relu),\n",
    "    Dense(units=3, activation=tf.nn.relu),\n",
    "    Dense(units=1, activation=None)\n",
    "])\n",
    "nn_model.compile(loss=BinaryCrossentropy(from_logits=True),optimizer='Adam',metrics=['Accuracy'])\n",
    "nn_model.fit(x_train,y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb31854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - Accuracy: 0.9732 - loss: 0.0808 \n"
     ]
    }
   ],
   "source": [
    "#Testing the accuracy of the neural network on the testing set:\n",
    "nn_loss, nn_acc = nn_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51ab0028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Original Model: 88.01874163319945\n",
      "Accuracy of Standardized Model: 89.7590361445783\n",
      "Accuracy of Neural Network Model: 97.38956093788147\n"
     ]
    }
   ],
   "source": [
    "#Overall Model Metrics\n",
    "\n",
    "print(\"Accuracy of Original Model: {acc}\".format(acc=init_acc))\n",
    "print(\"Accuracy of Standardized Model: {acc}\".format(acc=s_acc))\n",
    "print(\"Accuracy of Neural Network Model: {acc}\".format(acc=nn_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
